{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToxBlocker Model: Interactive Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NLP libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain_full = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = dfTrain_full.iloc[0:1000000 , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text  \\\n",
       "0  59848     0.0  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849     0.0  Thank you!! This would make my life a lot less...   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  \\\n",
       "0              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "1              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "\n",
       "[2 rows x 45 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7097320</td>\n",
       "      <td>[ Integrity means that you pay your debts.]\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7097321</td>\n",
       "      <td>This is malfeasance by the Administrator and t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text\n",
       "0  7097320  [ Integrity means that you pay your debts.]\\n\\...\n",
       "1  7097321  This is malfeasance by the Administrator and t..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data before feeding to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-7f88650c7aee>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfTrain['comment_text'] = dfTrain['comment_text'].str.replace(r'[^A-Za-z0-9]()?!@\\s\\'\\~\\*\\\"\\_\\n','')\n",
      "<ipython-input-29-7f88650c7aee>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfTrain['comment_text'] = dfTrain['comment_text'].str.replace(r'@','at')\n",
      "<ipython-input-29-7f88650c7aee>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfTrain['comment_text'] = dfTrain['comment_text'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "dfTrain['comment_text'] = dfTrain['comment_text'].str.replace(r'[^A-Za-z0-9]()?!@\\s\\'\\~\\*\\\"\\_\\n','')\n",
    "dfTrain['comment_text'] = dfTrain['comment_text'].str.replace(r'@','at')\n",
    "dfTrain['comment_text'] = dfTrain['comment_text'].str.lower()\n",
    "dfTest['comment_text'] = dfTest['comment_text'].str.replace(r'[^A-Za-z0-9]()?!@\\s\\'\\~\\*\\\"\\_\\n','')\n",
    "dfTest['comment_text'] = dfTest['comment_text'].str.replace(r'@','at')\n",
    "dfTest['comment_text'] = dfTest['comment_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    this is so cool. it's like, 'would you want yo...\n",
       "1    thank you!! this would make my life a lot less...\n",
       "2    this is such an urgent design problem; kudos t...\n",
       "3    is this something i'll be able to install on m...\n",
       "4                 haha you guys are a bunch of losers.\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain['comment_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-2680a898a6bc>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfTrain['toks'] = dfTrain['comment_text'].apply(tokenizer.tokenize)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+\\'*[a-zA-Z]+')\n",
    "dfTrain['toks'] = dfTrain['comment_text'].apply(tokenizer.tokenize)\n",
    "dfTest['toks'] = dfTest['comment_text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>toks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this is so cool. it's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[this, is, so, cool, it's, like, would, you, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text  \\\n",
       "0  59848     0.0  this is so cool. it's like, 'would you want yo...   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  \\\n",
       "0              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "\n",
       "   ...    rating  funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "0  ...  rejected      0    0    0      0         0              0.0   \n",
       "\n",
       "   identity_annotator_count  toxicity_annotator_count  \\\n",
       "0                         0                         4   \n",
       "\n",
       "                                                toks  \n",
       "0  [this, is, so, cool, it's, like, would, you, w...  \n",
       "\n",
       "[1 rows x 46 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-04f680db9828>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfTrain['no stopwords'] = dfTrain['toks'].apply(lambda x: [item for item in x if item not in sw])\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "dfTrain['no stopwords'] = dfTrain['toks'].apply(lambda x: [item for item in x if item not in sw])\n",
    "dfTest['no stopwords'] = dfTest['toks'].apply(lambda x: [item for item in x if item not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-a276413b92b8>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfTrain['speech_tags'] = dfTrain['no stopwords'].apply(lambda x: pos_tag(x))\n"
     ]
    }
   ],
   "source": [
    "dfTrain['speech_tags'] = dfTrain['no stopwords'].apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest['speech_tags'] = dfTest['no stopwords'].apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-10025493cc7f>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfTrain['wordnet_pos'] = dfTrain['speech_tags'].apply(lambda x: [(word,get_wordnet_pos(pos_tag)) for (word,pos_tag) in x])\n"
     ]
    }
   ],
   "source": [
    "dfTrain['wordnet_pos'] = dfTrain['speech_tags'].apply(lambda x: [(word,get_wordnet_pos(pos_tag)) for (word,pos_tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest['wordnet_pos'] = dfTest['speech_tags'].apply(lambda x: [(word,get_wordnet_pos(pos_tag)) for (word,pos_tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-b68f87f5a3b3>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfTrain['lemmatized'] = dfTrain['wordnet_pos'].apply(lambda x: [lem.lemmatize(word,tag) for (word,tag) in x])\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "dfTrain['lemmatized'] = dfTrain['wordnet_pos'].apply(lambda x: [lem.lemmatize(word,tag) for (word,tag) in x])\n",
    "dfTest['lemmatized'] = dfTest['wordnet_pos'].apply(lambda x: [lem.lemmatize(word,tag) for (word,tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  27382853\n",
      "Unique words:  222000\n",
      "Longest sentence:  305  words\n"
     ]
    }
   ],
   "source": [
    "words = [word for row in dfTrain['lemmatized'] for word in row]\n",
    "vocab = list(set(words))\n",
    "sentence_lengths = [len(sentence) for sentence in dfTrain['lemmatized']]\n",
    "print('Total words: ', len(words))\n",
    "print('Unique words: ', len(vocab))\n",
    "print('Longest sentence: ', max(sentence_lengths), ' words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "word2vec_path = \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" #\"~/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['toks'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, dfTrain)\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, dfTrain['target'], \n",
    "                                                                                        test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfTrain['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "VALIDATION_SPLIT=.2\n",
    "tokeniz = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokeniz.fit_on_texts(dfTrain['lemmatized'].tolist())\n",
    "sequences = tokeniz.texts_to_sequences(dfTrain['lemmatized'].tolist())\n",
    "\n",
    "word_index = tokeniz.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\nicol\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (1.41.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "sequence_length = MAX_SEQUENCE_LENGTH\n",
    "word_idx = 100  # len(word_index)\n",
    "\n",
    "inp = Input(shape=(sequence_length,))\n",
    "x = Embedding(len(word_index)+1, embedding_dim, weights=[embedding_weights], trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(64, recurrent_dropout=0.1))(x)\n",
    "x = Dense(32)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "y = Dense(1, activation='sigmoid')(x)\n",
    "NN2 = Model(inp,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.asarray(dfTrain[\"target\"])  # to_categorical(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 1000\n",
    "x_train = cnn_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = cnn_data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)\n",
    "np.asarray(dfTrain[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7805/7805 [==============================] - 968s 123ms/step - loss: 0.0000e+00 - acc: 0.7062 - val_loss: 0.0000e+00 - val_acc: 0.7400\n",
      "Epoch 2/3\n",
      "7805/7805 [==============================] - 898s 115ms/step - loss: 0.0000e+00 - acc: 0.7062 - val_loss: 0.0000e+00 - val_acc: 0.7400\n",
      "Epoch 3/3\n",
      "7805/7805 [==============================] - 897s 115ms/step - loss: 0.0000e+00 - acc: 0.7062 - val_loss: 0.0000e+00 - val_acc: 0.7400\n"
     ]
    }
   ],
   "source": [
    "NN2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "history = NN2.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokeniz.texts_to_sequences(dfTest['no stopwords'].tolist())\n",
    "NN2.predict(pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000201A9155310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000201A913E400> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "NN2.save('saved_model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"saved_model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_model.predict(pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output from short training run:\n",
    "Epoch 1/3\n",
    "7805/7805 [==============================] - 968s 123ms/step - loss: 0.0000e+00 - acc: 0.7062 - val_loss: 0.0000e+00 - val_acc: 0.7400\n",
    "Epoch 2/3\n",
    "7805/7805 [==============================] - 898s 115ms/step - loss: 0.0000e+00 - acc: 0.7062 - val_loss: 0.0000e+00 - val_acc: 0.7400\n",
    "Epoch 3/3\n",
    "7805/7805 [==============================] - 897s 115ms/step - loss: 0.0000e+00 - acc: 0.7062 - val_loss: 0.0000e+00 - val_acc: 0.7400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
